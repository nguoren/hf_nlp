{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e6UwkQKIO8K"
      },
      "source": [
        "# Models (TensorFlow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41HKXrKEIO8M"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tvMypTovIO8o"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, TFBertModel\n",
        "\n",
        "# Building the config\n",
        "config = BertConfig()\n",
        "\n",
        "# Building the model from the config\n",
        "model = TFBertModel(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G9KNWL-SIO8p",
        "outputId": "4a5f37b4-ce9f-435e-dbc6-db485300efbf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BertConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"classifier_dropout\": null,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"transformers_version\": \"4.50.0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_QVciHH1IO8r",
        "outputId": "67b44e79-4b11-4220-c4af-ba364f0f5944",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "model = TFBertModel.from_pretrained(\"bert-base-cased\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "faCxKwYpIO8v"
      },
      "outputs": [],
      "source": [
        "encoded_sequences = [\n",
        "    [101, 7592, 999, 102],\n",
        "    [101, 4658, 1012, 102],\n",
        "    [101, 3835, 999, 102],\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fcoWEMkpIO8w"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "model_inputs = tf.constant(encoded_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "mX0mvHAPIO8w"
      },
      "outputs": [],
      "source": [
        "output = model(model_inputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "id": "R9kMc726Nhcn",
        "outputId": "9c493811-13a3-44ed-e2bf-2594e03a8d88",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\n",
            "array([[[ 4.4495744e-01,  4.8276284e-01,  2.7797198e-01, ...,\n",
            "         -5.4032274e-02,  3.9393425e-01, -9.4769992e-02],\n",
            "        [ 2.4942899e-01, -4.4092962e-01,  8.1772339e-01, ...,\n",
            "         -3.1916568e-01,  2.2992219e-01, -4.1171715e-02],\n",
            "        [ 1.3667546e-01,  2.2517854e-01,  1.4501992e-01, ...,\n",
            "         -4.6914592e-02,  2.8224233e-01,  7.5565845e-02],\n",
            "        [ 1.1788858e+00,  1.6738570e-01, -1.8187097e-01, ...,\n",
            "          2.4671350e-01,  1.0440764e+00, -6.1971312e-03]],\n",
            "\n",
            "       [[ 3.6435828e-01,  3.2463297e-02,  2.0257683e-01, ...,\n",
            "          6.0110353e-02,  3.2451293e-01, -2.0995583e-02],\n",
            "        [ 7.1865958e-01, -4.8725206e-01,  5.1740402e-01, ...,\n",
            "         -4.4012007e-01,  1.4553043e-01, -3.7544727e-02],\n",
            "        [ 3.3223230e-01, -2.3270901e-01,  9.4876081e-02, ...,\n",
            "         -2.5268167e-01,  3.2172003e-01,  8.1142318e-04],\n",
            "        [ 1.2523208e+00,  3.5754293e-01, -5.1321149e-02, ...,\n",
            "         -3.7839708e-01,  1.0526475e+00, -5.6254762e-01]],\n",
            "\n",
            "       [[ 2.4042264e-01,  1.4717777e-01,  1.2110285e-01, ...,\n",
            "          7.6061681e-02,  3.3564472e-01,  2.8261665e-01],\n",
            "        [ 6.5700614e-01, -3.2786581e-01,  2.4967620e-01, ...,\n",
            "         -2.5919506e-01,  2.0174681e-01,  3.3275127e-01],\n",
            "        [ 2.0159575e-01,  1.5782663e-01,  9.8975692e-03, ...,\n",
            "         -3.8850459e-01,  4.1307470e-01,  3.9731947e-01],\n",
            "        [ 1.0174985e+00,  6.4386743e-01, -7.8146642e-01, ...,\n",
            "         -4.2109150e-01,  1.0925050e+00, -4.8456412e-02]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
            "array([[-0.6856277 ,  0.5262493 ,  0.99995273, ...,  0.99998754,\n",
            "        -0.61123455,  0.9970657 ],\n",
            "       [-0.60545814,  0.49971724,  0.99981916, ...,  0.9999408 ,\n",
            "        -0.67532754,  0.97692657],\n",
            "       [-0.77015084,  0.54474187,  0.9999417 , ...,  0.9999845 ,\n",
            "        -0.4654935 ,  0.98939013]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "v1tdTn33IO8t"
      },
      "outputs": [],
      "source": [
        "sequences = [\"Hello!\", \"Cool.\", \"Nice!\"]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "input = tokenizer(sequences, padding=True, truncation=True)\n",
        "print(input)"
      ],
      "metadata": {
        "id": "hmW22zIhPjLY",
        "outputId": "d795ed00-a2e5-4015-8775-de5e2492e306",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 8667, 106, 102], [101, 13297, 119, 102], [101, 8835, 106, 102]], 'token_type_ids': [[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_input_id = tf.constant(input[\"input_ids\"])\n",
        "model_attention_mask = tf.constant(input[\"attention_mask\"])\n",
        "\n",
        "output2 = model(model_input_id, model_attention_mask)\n",
        "print(output2)"
      ],
      "metadata": {
        "id": "8lzohxPaNiQE",
        "outputId": "a405097c-be25-401c-f59d-73ba454b4679",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor: shape=(3, 4, 768), dtype=float32, numpy=\n",
            "array([[[ 0.62828964,  0.21656758,  0.560512  , ...,  0.01361646,\n",
            "          0.615793  , -0.17120235],\n",
            "        [ 0.6108095 , -0.22526626,  0.92628855, ..., -0.30280808,\n",
            "          0.4499951 , -0.07135769],\n",
            "        [ 0.8039674 ,  0.18094489,  0.70756704, ..., -0.0684973 ,\n",
            "          0.48369724, -0.07738485],\n",
            "        [ 1.3290284 ,  0.2359512 ,  0.4566581 , ...,  0.1508819 ,\n",
            "          0.9621055 , -0.48411724]],\n",
            "\n",
            "       [[ 0.31276435,  0.17181471,  0.20987779, ..., -0.0721087 ,\n",
            "          0.4918776 , -0.13833432],\n",
            "        [ 0.15445176, -0.37572706,  0.7187129 , ..., -0.31295148,\n",
            "          0.2821989 ,  0.18830812],\n",
            "        [ 0.41229045,  0.37207627,  0.54835176, ...,  0.07883389,\n",
            "          0.56807595, -0.27571857],\n",
            "        [ 0.8356334 ,  0.39642993, -0.41206455, ...,  0.183796  ,\n",
            "          1.6364969 , -0.4806332 ]],\n",
            "\n",
            "       [[ 0.53993607,  0.25642586,  0.25112194, ..., -0.1760174 ,\n",
            "          0.6062912 , -0.18034875],\n",
            "        [ 0.2609487 , -0.31643322,  0.5547665 , ..., -0.3438956 ,\n",
            "          0.39093736,  0.09002222],\n",
            "        [ 0.51608056,  0.07206444,  0.5605513 , ...,  0.0076579 ,\n",
            "          0.3684777 , -0.22716518],\n",
            "        [ 0.6559557 ,  0.8475468 , -0.16064113, ..., -0.04676463,\n",
            "          1.6308942 , -0.5046526 ]]], dtype=float32)>, pooler_output=<tf.Tensor: shape=(3, 768), dtype=float32, numpy=\n",
            "array([[-0.71049666,  0.4876317 ,  0.9998759 , ...,  0.9999613 ,\n",
            "        -0.91794103,  0.9893576 ],\n",
            "       [-0.77313524,  0.5618816 ,  0.9999569 , ...,  0.9999864 ,\n",
            "        -0.8397392 ,  0.9943837 ],\n",
            "       [-0.75936025,  0.5644549 ,  0.99997395, ...,  0.9999917 ,\n",
            "        -0.9015355 ,  0.9969215 ]], dtype=float32)>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7d1zIcevPDU6"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Models (TensorFlow)",
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}